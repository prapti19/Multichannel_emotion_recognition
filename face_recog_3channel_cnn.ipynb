{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"colab":{"name":"ck_97.26.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"QxxcV7Geygva","colab_type":"text"},"source":["# 1. Import All Required Packages"]},{"cell_type":"code","metadata":{"id":"tpEg-SABygve","colab_type":"code","colab":{}},"source":["# Import libraries# Impor \n","import os,cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","from pylab import rcParams\n","rcParams['figure.figsize'] = 20, 10\n","\n","from sklearn.utils import shuffle\n","from sklearn.cross_validation import train_test_split\n","\n","import keras\n","\n","from keras.utils import np_utils\n","\n","from keras import backend as K\n","from keras.layers import LSTM\n","from keras.layers import TimeDistributed\n","\n","from keras.models import Sequential\n","from keras.layers.core import Dense, Dropout, Activation, Flatten\n","from keras.layers.convolutional import Convolution2D, MaxPooling2D\n","from keras.optimizers import SGD,RMSprop,adam\n","from keras.preprocessing.image import ImageDataGenerator"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jPv_wZB1ygvp","colab_type":"text"},"source":["# 2.Read Images from the data path"]},{"cell_type":"code","metadata":{"id":"gA4_iTkeygvr","colab_type":"code","colab":{},"outputId":"f6467975-8543-4bff-acd6-80f0f1a7d478"},"source":["#Define Datapath\n","data_path = './ck_crop_gauss/'\n","data_dir_list = os.listdir(data_path)\n","\n","img_rows=256\n","img_cols=256\n","num_channel=1\n","\n","num_epoch=10\n","\n","img_data_list=[]\n","\n","\n","for dataset in data_dir_list:\n","    img_list=os.listdir(data_path+'/'+ dataset)\n","    print ('Loaded the images of dataset-'+'{}\\n'.format(dataset))\n","    for img in img_list:\n","        input_img=cv2.imread(data_path + '/'+ dataset + '/'+ img )\n","        #input_img=cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n","        input_img_resize=cv2.resize(input_img,(128,128))\n","        img_data_list.append(input_img_resize)\n","        \n","img_data = np.array(img_data_list)\n","img_data = img_data.astype('float32')\n","\n","\n","img_data = img_data/255\n","img_data.shape"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loaded the images of dataset-anger\n","\n","Loaded the images of dataset-disgust\n","\n","Loaded the images of dataset-fear\n","\n","Loaded the images of dataset-happiness\n","\n","Loaded the images of dataset-neutral\n","\n","Loaded the images of dataset-sadness\n","\n","Loaded the images of dataset-surprise\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(902, 128, 128, 3)"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"rJxA--0Yygv3","colab_type":"text"},"source":["# 3.Define the number of classes"]},{"cell_type":"code","metadata":{"id":"UpcyJACbygv5","colab_type":"code","colab":{}},"source":["num_classes = 7\n","num_of_samples = img_data.shape[0]\n","labels = np.ones((num_of_samples,),dtype='int64')\n","labels[0:45]=0 #30\n","labels[46:105]=1 #29\n","labels[106:131]=2 #32\n","labels[132:201]=3 #31\n","labels[202:795]=4 #30\n","labels[796:824]=5 #31\n","labels[825:]=6 #30\n","names = ['angry','disgust','fear','happiness','neutral','sadness','surprise']\n","def getLabel(id):\n","    return ['angry','disgust','fear','happiness','neutral','sadness','surprise'][id]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bg2hz9FVygwC","colab_type":"text"},"source":["# 4.Convert the class label to one-hot encoding"]},{"cell_type":"code","metadata":{"id":"0RDTzKk7ygwD","colab_type":"code","colab":{},"outputId":"fec52ab4-567b-4819-a493-e9600d1e9191"},"source":["# convert class labels to on-hot encoding# conve \n","Y = np_utils.to_categorical(labels, num_classes)\n","\n","#Shuffle the dataset\n","x,y = shuffle(img_data,Y, random_state=2)\n","# Split the dataset\n","X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=2)\n","print(img_data[0].shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(128, 128, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"N28sX8_EygwI","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Odhn-PiqygwM","colab_type":"code","colab":{},"outputId":"9b434ec2-2193-4337-93a6-e51d01f6e295"},"source":["#Define Datapath\n","data_path = './ck_crop_gauss/'\n","data_dir_list = os.listdir(data_path)\n","\n","img_rows=256\n","img_cols=256\n","num_channel=1\n","\n","num_epoch=10\n","\n","img_data_list=[]\n","\n","\n","for dataset in data_dir_list:\n","    img_list=os.listdir(data_path+'/'+ dataset)\n","    print ('Loaded the images of dataset-'+'{}\\n'.format(dataset))\n","    for img in img_list:\n","        input_img=cv2.imread(data_path + '/'+ dataset + '/'+ img )\n","        #input_img=cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n","        input_img_resize=cv2.resize(input_img,(128,128))\n","        img_data_list.append(input_img_resize)\n","        \n","img_data = np.array(img_data_list)\n","img_data = img_data.astype('float32')\n","\n","\n","img_data = img_data/255\n","img_data.shape"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loaded the images of dataset-anger\n","\n","Loaded the images of dataset-disgust\n","\n","Loaded the images of dataset-fear\n","\n","Loaded the images of dataset-happiness\n","\n","Loaded the images of dataset-neutral\n","\n","Loaded the images of dataset-sadness\n","\n","Loaded the images of dataset-surprise\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(902, 128, 128, 3)"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"WtkuaL-dygwT","colab_type":"code","colab":{}},"source":["num_classes = 7\n","num_of_samples = img_data.shape[0]\n","labels = np.ones((num_of_samples,),dtype='int64')\n","labels[0:45]=0 #30\n","labels[46:105]=1 #29\n","labels[106:131]=2 #32\n","labels[132:201]=3 #31\n","labels[202:795]=4 #30\n","labels[796:824]=5 #31\n","labels[825:]=6 #30\n","names = ['angry','disgust','fear','happiness','neutral','sadness','surprise']\n","def getLabel(id):\n","    return ['angry','disgust','fear','happiness','neutral','sadness','surprise'][id]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zXNgbvSSygwY","colab_type":"code","colab":{},"outputId":"133ae34b-b89d-4afd-accb-763e6f7acaf1"},"source":["# convert class labels to on-hot encoding# conve \n","Y = np_utils.to_categorical(labels, num_classes)\n","\n","#Shuffle the dataset\n","x,y = shuffle(img_data,Y, random_state=2)\n","# Split the dataset\n","X2_train, X2_test, y2_train, y2_test = train_test_split(x, y, test_size=0.15, random_state=2)\n","print(img_data[0].shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(128, 128, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"plieSdp1ygwd","colab_type":"code","colab":{},"outputId":"f4e6e09c-23f5-445f-bff3-ac8197cba1e9"},"source":["#Define Datapath\n","data_path = './ck_crop_sobel/'\n","data_dir_list = os.listdir(data_path)\n","\n","img_rows=256\n","img_cols=256\n","num_channel=1\n","\n","num_epoch=10\n","\n","img_data_list=[]\n","\n","\n","for dataset in data_dir_list:\n","    img_list=os.listdir(data_path+'/'+ dataset)\n","    print ('Loaded the images of dataset-'+'{}\\n'.format(dataset))\n","    for img in img_list:\n","        input_img=cv2.imread(data_path + '/'+ dataset + '/'+ img )\n","        #input_img=cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n","        input_img_resize=cv2.resize(input_img,(128,128))\n","        img_data_list.append(input_img_resize)\n","        \n","img_data = np.array(img_data_list)\n","img_data = img_data.astype('float32')\n","\n","\n","img_data = img_data/255\n","img_data.shape"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loaded the images of dataset-anger\n","\n","Loaded the images of dataset-disgust\n","\n","Loaded the images of dataset-fear\n","\n","Loaded the images of dataset-happiness\n","\n","Loaded the images of dataset-neutral\n","\n","Loaded the images of dataset-sadness\n","\n","Loaded the images of dataset-surprise\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(902, 128, 128, 3)"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"0NT5wui1ygwj","colab_type":"code","colab":{}},"source":["num_classes = 7\n","num_of_samples = img_data.shape[0]\n","labels = np.ones((num_of_samples,),dtype='int64')\n","labels[0:45]=0 #30\n","labels[46:105]=1 #29\n","labels[106:131]=2 #32\n","labels[132:201]=3 #31\n","labels[202:795]=4 #30\n","labels[796:824]=5 #31\n","labels[825:]=6 #30\n","names = ['angry','disgust','fear','happiness','neutral','sadness','surprise']\n","def getLabel(id):\n","    return ['angry','disgust','fear','happiness','neutral','sadness','surprise'][id]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ov8ALXoJygwp","colab_type":"code","colab":{},"outputId":"3dc6323d-c118-4632-e98e-46a8f8093704"},"source":["# convert class labels to on-hot encoding# conve \n","Y = np_utils.to_categorical(labels, num_classes)\n","\n","#Shuffle the dataset\n","x,y = shuffle(img_data,Y, random_state=2)\n","# Split the dataset\n","X3_train, X3_test, y3_train, y3_test = train_test_split(x, y, test_size=0.15, random_state=2)\n","print(img_data[0].shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(128, 128, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lxqBAtpkygwu","colab_type":"code","colab":{}},"source":["from keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten\n","from keras.models import Model\n","\n","# First, define the vision modules\n","digit_input = Input(shape=(128,128,3))\n","x = Conv2D(32, (3, 3))(digit_input)\n","#x= PReLU(shared_axes=[1,2],name='prelu2')(x)\n","x = MaxPooling2D((2, 2))(x)\n","x = Conv2D(64, (3, 3))(x)\n","#x= PReLU(shared_axes=[1,2],name='prelu2')(x)\n","x = MaxPooling2D((2, 2))(x)\n","x = Conv2D(64, (3, 3))(x)\n","#x= PReLU(shared_axes=[1,2],name='prelu2')(x)\n","x = MaxPooling2D((2, 2))(x)\n","x = Conv2D(128, (2, 2))(x)\n","#x= PReLU(shared_axes=[1,2],name='prelu2')(x)\n","x = MaxPooling2D((2, 2))(x)\n","out = Flatten()(x)\n","\n","vision_model = Model(digit_input, out)\n","\n","# Then define the tell-digits-apart model\n","digit_a = Input(shape=(128, 128, 3))\n","digit_b = Input(shape=(128, 128, 3))\n","digit_c = Input(shape=(128, 128, 3))\n","digit_d = Input(shape=(128, 128, 3))\n","\n","# The vision model will be shared, weights and all\n","out_a = vision_model(digit_a)\n","out_b = vision_model(digit_b)\n","out_c = vision_model(digit_c)\n","out_d = vision_model(digit_d)\n","\n","concatenated = keras.layers.concatenate([out_a, out_b, out_c])\n","out = Dense(7, activation='sigmoid')(concatenated)\n","\n","classification_model = Model([digit_a, digit_b, digit_c], out)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H5aKDopBygwz","colab_type":"code","colab":{}},"source":["classification_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zAwKZyn2ygw3","colab_type":"code","colab":{}},"source":["from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FvWDyg9Jygw7","colab_type":"code","colab":{}},"source":["epochs = 30\n","batch_size = 7\n","\n","callbacks = [\n","    EarlyStopping(patience=3, verbose=1),\n","    ReduceLROnPlateau(patience=3, verbose=1)\n","]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qgsAPrnrygw_","colab_type":"code","colab":{},"outputId":"c6932891-f2a8-4bea-da91-fb760a872294"},"source":["history = classification_model.fit([X_train,X2_train,X3_train], y_train, \n","                    batch_size=batch_size, epochs=epochs, callbacks=callbacks,\n","                    validation_data=([X_test, X2_test,X3_test], y_test),\n","                    verbose=1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 766 samples, validate on 136 samples\n","Epoch 1/30\n","766/766 [==============================] - 71s 92ms/step - loss: 0.0275 - acc: 0.9905 - val_loss: 0.1241 - val_acc: 0.9716\n","Epoch 2/30\n","766/766 [==============================] - 69s 90ms/step - loss: 0.0184 - acc: 0.9937 - val_loss: 0.1498 - val_acc: 0.9706\n","Epoch 3/30\n","766/766 [==============================] - 70s 91ms/step - loss: 0.0083 - acc: 0.9976 - val_loss: 0.1797 - val_acc: 0.9643\n","Epoch 4/30\n","766/766 [==============================] - 69s 90ms/step - loss: 0.0110 - acc: 0.9961 - val_loss: 0.1531 - val_acc: 0.9727\n","\n","Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","Epoch 00004: early stopping\n"],"name":"stdout"}]}]}